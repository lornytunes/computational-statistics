{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Testing\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Callable\n",
    "from functools import partial\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')\n",
    "\n",
    "import nsfg\n",
    "from pmf import Pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "sns.set_theme()\n",
    "figsize(11, 5)\n",
    "\n",
    "# some nicer colors from http://colorbrewer2.org/\n",
    "COLOR1 = '#7fc97f'\n",
    "COLOR2 = '#beaed4'\n",
    "COLOR3 = '#fdc086'\n",
    "COLOR4 = '#ffff99'\n",
    "COLOR5 = '#386cb0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = partial(np.round, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesisTest(ABC):\n",
    "    '''\n",
    "    A class that represents the structure of a classical hypothesis test\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data: List[float]):\n",
    "        self.data = data\n",
    "        self.make_model()\n",
    "        self.actual = self.test_statistic(data)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def test_statistic(self, data: List[float]) -> float:\n",
    "        '''\n",
    "        Provides the test statistic of interest\n",
    "        '''\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_model(self):\n",
    "        '''\n",
    "        Sets up the test\n",
    "        '''\n",
    "        \n",
    "    @abstractmethod\n",
    "    def run_model(self) -> List[float]:\n",
    "        '''\n",
    "        Runs the test - generates the data to pass to test_statistic\n",
    "        '''\n",
    "        \n",
    "    def p_value(self, iters=1000):\n",
    "        '''\n",
    "        Computes the p-value\n",
    "        '''\n",
    "        self.test_stats = np.array([\n",
    "            self.test_statistic(self.run_model()) for _ in range(iters)\n",
    "        ])\n",
    "        # proportion of stats greater than the actual value\n",
    "        return sum(self.test_stats >= self.actual) / iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, suppose we toss a coin 250 times and see 140 heads and 110 tails. Based on this result, we might suspect that the coin is biased; that is, more likely to land heads. To test this hypothesis, we compute the probability of seeing such a difference if the coin is actually fair:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinTest(HypothesisTest):\n",
    "    \n",
    "    def test_statistic(self, data):\n",
    "        # the absolute difference between the number\n",
    "        # of heads and tails\n",
    "        heads, tails = data\n",
    "        return abs(heads - tails)\n",
    "    \n",
    "    def make_model(self):\n",
    "        # nothing to do\n",
    "        pass\n",
    "    \n",
    "    def run_model(self):\n",
    "        '''\n",
    "        Simulates coin tosses assuming that the coin is actually fair\n",
    "        '''\n",
    "        # heads + tails\n",
    "        n = sum(self.data)\n",
    "        # generate a sample of n coin tosses\n",
    "        sample = np.random.binomial(1, 0.5, n)\n",
    "        # return (number of heads, number of tails)\n",
    "        return (sum(sample == 1), sum(sample == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw from (0, 1) 10 times with probablity of 0.5 for each\n",
    "np.random.binomial(1, 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CoinTest((140, 110))\n",
    "print(f'P value: {ct.p_value(iters=1000):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is about 0.07, which means that if the coin is fair, we expect to see a difference as big as 30 about 7% of the time.\n",
    "\n",
    "How should we interpret this result? By convention, 5% is the threshold of statistical significance. If the p-value is less than 5%, the effect is considered significant; otherwise it is not.\n",
    "\n",
    "But the choice of 5% is arbitrary, and (as we will see later) the p-value depends on the choice of the test statistics and the model of the null hypothesis. So p-values should not be considered precise measurements.\n",
    "\n",
    "I recommend interpreting p-values according to their order of magnitude: if the p-value is less than 1%, the effect is unlikely to be due to chance; if it is greater than 10%, the effect can plausibly be explained by chance. P-values between 1% and 10% should be considered borderline. So in this example I conclude that the data do not provide strong evidence that the coin is biased or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example that tests whether the outcome of a rolling a six-sided die is suspicious, where the test statistic is the total absolute difference between the observed outcomes and the expected long-term averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceTest(HypothesisTest):\n",
    "    \n",
    "    FACES = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    def test_statistic(self, data):\n",
    "        # data is what has been observed\n",
    "        n = sum(data)\n",
    "        # array of 1/6 values the equal to the number of observations\n",
    "        expected = np.ones(6) * n / 6\n",
    "        return sum(abs(data - expected))\n",
    "    \n",
    "    def make_model(self):\n",
    "        pass\n",
    "\n",
    "    def run_model(self):\n",
    "        n = sum(self.data)\n",
    "        rolls = np.random.choice(self.FACES, n, replace=True)\n",
    "        hist = Counter(rolls)\n",
    "        # the frequencies are the values. return them in order of the dice values 1-6\n",
    "        return np.array([hist[i] for i in self.FACES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example using the data from the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([8, 9, 19, 5, 8, 11])\n",
    "dt = DiceTest(data)\n",
    "pvalue = dt.p_value(iters=10000)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed deviance from the expected values is not statistically significant.\n",
    "\n",
    "By convention, it is more common to test data like this using the chi-squared statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceChiTest(DiceTest):\n",
    "\n",
    "    def test_statistic(self, data):\n",
    "        n = sum(data)\n",
    "        expected = np.ones(6) * n / 6\n",
    "        return sum((data - expected)**2 / expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DiceChiTest(data)\n",
    "pvalue = dt.p_value(iters=10000)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this result at face value, we might consider the data statistically significant, but considering the results of both tests, I would not draw any strong conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a difference in means\n",
    "\n",
    "One of the most common effects to test is a difference in mean between two groups. In the NSFG data, the mean pregnancy length for first babies is slightly longer, and the mean birth weight is slightly smaller. Now we will see if those effects are statistically significant.\n",
    "\n",
    "For these examples, the null hypothesis is that the distributions for the two groups are the same. One way to model the null hypothesis is by permutation; that is, we can take values for first babies and others and shuffle them, treating the two groups as one big group:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you observe an apparent difference between two groups and you want to check whether it might be due to chance.\n",
    "\n",
    "As an example, we'll look at differences between first babies and others.  The `first` module provides code to read data from the National Survey of Family Growth (NSFG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pregnency data - live births only\n",
    "live = nsfg.read_live_fem_preg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.birthord.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize as firsts and others\n",
    "live.birthcat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interested in prglngth\n",
    "live.prglngth.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missing values\n",
    "live.prglngth.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition first babies and others\n",
    "firsts = live[live.birthcat == 'firsts'].prglngth\n",
    "others = live[live.birthcat == 'others'].prglngth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weeks2hours(weeks: np.float64) -> np.float64:\n",
    "    '''\n",
    "    Convert long time units (weeks) into short (hours)\n",
    "    '''\n",
    "    return weeks * 7 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at a couple of variables, including pregnancy length and birth weight.  The effect size we'll consider is the difference in the means.\n",
    "\n",
    "Other examples might include a correlation between variables or a coefficient in a linear regression.  The number that quantifies the size of the effect is called the \"test statistic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMeansPermute(HypothesisTest):\n",
    "    \n",
    "    def test_statistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = abs(group1.mean() - group2.mean())\n",
    "        return test_stat\n",
    "    \n",
    "    def make_model(self):\n",
    "        group1, group2 = self.data\n",
    "        self.n, self.m = len(group1), len(group2)\n",
    "        self.pool = np.hstack((group1, group2))\n",
    "        \n",
    "    def run_model(self):\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.m:]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = DiffMeansPermute((firsts, others,))\n",
    "pvalue = ht.p_value()\n",
    "print(f'p value: {pvalue:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMeansOneSided(DiffMeansPermute):\n",
    "    \n",
    "    def test_statistic(self, data):\n",
    "        group1, group2 = data\n",
    "        return group1.mean() - group2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = DiffMeansOneSided((firsts, others,))\n",
    "pvalue = ht.p_value()\n",
    "print(f'p value: {pvalue:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks2hours(ht.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ht, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold our two groups\n",
    "\n",
    "@dataclass\n",
    "class GroupPair:\n",
    "    \n",
    "    group1: np.array\n",
    "    group2: np.array\n",
    "    \n",
    "    @property\n",
    "    def lengths(self) -> Tuple[int, int]:\n",
    "        return (len(self.group1), len(self.group2))\n",
    "    \n",
    "    @property\n",
    "    def means(self) -> Tuple[np.float64, np.float64]:\n",
    "        return (self.group1.mean(), self.group2.mean())\n",
    "    \n",
    "# group stats\n",
    "def mean_diff(gp: GroupPair) -> np.float64:\n",
    "    return abs(gp.group1.mean() - gp.group2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first example, I extract the pregnancy length for first babies and others.  The results are pandas Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GroupPair(\n",
    "    firsts.values,\n",
    "    others.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = mean_diff(data)\n",
    "print(f'Actual difference: {weeks2hours(actual):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual difference in the means is 0.078 weeks, which is only 13 hours.\n",
    "\n",
    "The null hypothesis is that there is no difference between the groups.  We can model that by forming a pooled sample that includes first babies and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_sample(data: GroupPair) -> GroupPair:\n",
    "    # represents null hypothesis\n",
    "    n, m = data.lengths\n",
    "    pool = np.hstack((data.group1, data.group2))\n",
    "    # shuffle the pool\n",
    "    np.random.shuffle(pool)\n",
    "    # return as a new grouped pair using the same sizes as the actual sample\n",
    "    return GroupPair(pool[:n], pool[n:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simulate the null hypothesis by shuffling the pool and dividing it into two groups, using the same sizes as the actual sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of running the model is two NumPy arrays with the shuffled pregnancy lengths:\n",
    "\n",
    "Then we compute the same test statistic using the simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pooled_sample(data)\n",
    "r2(weeks2hours(mean_diff(sample_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the model 1000 times and compute the test statistic, we can see how much the test statistic varies under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data: GroupPair, test_stat: Callable, sampler: Callable, niters: int = 1000) -> np.ndarray:\n",
    "    return np.array([\n",
    "        test_stat(sampler(data)) for i in range(niters)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(test_stats: np.ndarray, actual: np.float64) -> np.float64:\n",
    "    '''\n",
    "    the proportion of differences that exceed the observed difference\n",
    "    '''\n",
    "    return sum(test_stats >= actual) / len(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(test_stats: np.ndarray, actual: np.float64, label=None, title=None, bins=None):\n",
    "    \"\"\"\n",
    "    Draws a histogram with vertical lines at the observed test stat.\n",
    "    \"\"\"\n",
    "    args = {}\n",
    "    if bins is not None:\n",
    "        args['bins'] = bins\n",
    "    p = sns.histplot(\n",
    "        x = test_stats,\n",
    "        **args\n",
    "    )\n",
    "    p.axvline(actual, linewidth=2, color='darkred', linestyle='--')\n",
    "    p.set(\n",
    "        xlabel = label and label or 'test statistic',\n",
    "        ylabel = 'count'\n",
    "    );\n",
    "    if title:\n",
    "        p.set(title=f'{title}: Actual: {actual:.2f}')\n",
    "    else:\n",
    "        p.set(title=f'Actual: {actual:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the top\n",
    "actual = mean_diff(data)\n",
    "test_stats = run_model(data, mean_diff, sampler=pooled_sample, niters = 1000)\n",
    "p_val = p_value(test_stats, actual)\n",
    "print(f'P val: {p_val:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the sampling distribution of the test statistic under the null hypothesis, with the actual difference in means indicated by a gray line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    test_stats,\n",
    "    actual,\n",
    "    'Difference in means (weeks)',\n",
    "    title=f'p value: {p_val:0.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is the probability that the test statistic under the null hypothesis exceeds the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the result is about 15-17%, which means that even if there is no difference between the groups, it is plausible that we could see a sample difference as big as 0.078 weeks.\n",
    "\n",
    "We conclude that the apparent effect might be due to chance, so we are not confident that it would appear in the general population, or in another sample from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other test statistics\n",
    "\n",
    "Choosing the best test statistic depends on what question you are trying to address. For example, if the relevant question is whether pregnancy lengths are different for first babies, then it makes sense to test the absolute difference in means, as we did in the previous section.\n",
    "\n",
    "If we had some reason to think that first babies are likely to be late, then we would not take the absolute value of the difference; instead we would use this test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_diff_one_sided(gp: GroupPair) -> np.float64:\n",
    "    return gp.group1.mean() - gp.group2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the only difference is that `mean_diff` does not take the absolute value of the difference. This kind of test is called one-sided because it only counts one side of the distribution of differences. The previous test, using both sides, is _two-sided_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = mean_diff_one_sided(data)\n",
    "test_stats = run_model(data, mean_diff_one_sided, sampler=pooled_sample)\n",
    "p_val = p_value(test_stats, actual)\n",
    "print(f'One sided p value is: {p_val:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this version of the test, the p-value is 0.09. In general the p-value for a one-sided test is about half the p-value for a two-sided test, depending on the shape of the distribution.\n",
    "\n",
    "The one-sided hypothesis, that first babies are born late, is more specific than the two-sided hypothesis, so the p-value is smaller. But even for the stronger hypothesis, the difference is not statistically significant.\n",
    "\n",
    "We can use the same framework to test for a difference in standard deviation. There is some evidence that first babies are more likely to be early or late, and less likely to be on time. (TODO: show this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(\n",
    "    data=live,\n",
    "    x='prglngth',\n",
    "    hue='birthcat',\n",
    "    multiple='dodge'\n",
    ");\n",
    "p.set(\n",
    "    xlabel='Pregnancy length (weeks)',\n",
    "    ylabel='Count'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.kdeplot(\n",
    "    data=live,\n",
    "    x='prglngth',\n",
    "    hue='birthcat'\n",
    ");\n",
    "p.set(\n",
    "    xlabel='Pregnancy length (weeks)',\n",
    "    ylabel='Density'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.groupby('birthcat')['prglngth'].agg(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we might hypothesize that the standard deviation is higher. Here’s how we can test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_diff_one_sided(gp: GroupPair) -> np.float64:\n",
    "    return gp.group1.std() - gp.group2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = std_diff_one_sided(data)\n",
    "test_stats = run_model(data, std_diff_one_sided, pooled_sample)\n",
    "p_val = p_value(test_stats, actual)\n",
    "print(f'One sided p value for std is: {p_val:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    test_stats, actual, title=f'p value is {p_val:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a one-sided test because the hypothesis is that the standard deviation for first babies is higher, not just different. The p-value is 0.09, which is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a correlation\n",
    "\n",
    "This framework can also test correlations. For example, in the NSFG data set, the correlation between birth weight and mother’s age is about 0.07. It seems like older mothers have heavier babies. But could this effect be due to chance?\n",
    "\n",
    "For the test statistic, I use Pearson’s correlation, but Spearman’s would work as well. If we had reason to expect positive correlation, we would do a one-sided test. But since we have no such reason, I’ll do a two-sided test using the absolute value of correlation.\n",
    "\n",
    "The null hypothesis is that there is no correlation between mother’s age and birth weight. By shuffling the observed values, we can simulate a world where the distributions of age and birth weight are the same, but where the variables are unrelated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.agepreg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.loc[:, ['caseid', 'prglngth','totalwgt_lb', 'agepreg']].apply(lambda col: np.sum(col.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.totalwgt_lb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.dropna(subset=['totalwgt_lb'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(live.agepreg.values, live.totalwgt_lb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov(xs: np.array, ys: np.array) -> np.float64:\n",
    "    return np.dot(xs-xs.mean(), ys-ys.mean()) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(live.loc[:, ['agepreg', 'totalwgt_lb']].to_numpy(), rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov(live.agepreg, live.totalwgt_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor(xs: np.ndarray, ys: np.ndarray) -> np.float64:\n",
    "    return cov(xs, ys) / np.sqrt(xs.var() * ys.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor(live.agepreg, live.totalwgt_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stat\n",
    "def cor_test_stat(gp: GroupPair) -> np.float64:\n",
    "    return abs(cor(gp.group1, gp.group2))\n",
    "\n",
    "# null hypothesis\n",
    "def cor_sample(gp: GroupPair):\n",
    "    return GroupPair(\n",
    "        np.random.permutation(gp.group1),\n",
    "        gp.group2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GroupPair(\n",
    "    live.agepreg.values,\n",
    "    live.totalwgt_lb.values\n",
    ")\n",
    "actual = cor(data.group1, data.group2)\n",
    "test_stats = run_model(data, cor_test_stat, cor_sample, niters=10000)\n",
    "p_val = p_value(test_stats, actual)\n",
    "print(f'P value: {p_val:0.3f}, Maximum simulated correlation is {max(test_stats):0.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual correlation is 0.07. The computed p-value is 0; after 10000 iterations the largest simulated correlation is 0.04. So although the observed correlation is small, it is statistically significant.\n",
    "\n",
    "This example is a reminder that “statistically significant” does not always mean that an effect is important, or significant in practice. It only means that it is unlikely to have occurred by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    test_stats,\n",
    "    actual,\n",
    "    label='correlation',\n",
    "    title=f'p value for correlation test between pregancy age and birth weight is {p_val:0.2f}'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove global variables\n",
    "del data, actual, test_stats, p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, after 10000 attempts, we never see a sample difference as big as the observed difference, so we conclude that the apparent effect is unlikely under the null hypothesis.  Under normal circumstances, we can also make the inference that the apparent effect is unlikely to be caused by random sampling.\n",
    "\n",
    "One final note: in this case I would report that the p-value is less than 1/1000 or less than 0.001.  I would not report p=0, because  the apparent effect is not impossible under the null hypothesis; just unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors\n",
    "\n",
    "In this section, we'll explore the dangers of p-hacking by running multiple tests until we find one that's statistically significant.\n",
    "\n",
    "Suppose we want to compare IQs for two groups of people.  And suppose that, in fact, the two groups are statistically identical; that is, their IQs are drawn from a normal distribution with mean 100 and standard deviation 15.\n",
    "\n",
    "I'll use `numpy.random.normal` to generate fake data I might get from running such an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = GroupPair(\n",
    "    np.random.normal(100, 15, size=100),\n",
    "    np.random.normal(100, 15, size=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the mean in both groups to be near 100, but just by random chance, it might be higher or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2(groups.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use DiffMeansPermute to compute the p-value for this fake data, which is the probability that we would see a difference between the groups as big as what we saw, just by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = GroupPair(\n",
    "    np.random.normal(100, 15, size=100),\n",
    "    np.random.normal(100, 15, size=100)\n",
    ")\n",
    "actual = mean_diff(groups)\n",
    "test_stats = run_model(groups, mean_diff, pooled_sample, niters=1000)\n",
    "p_val = p_value(test_stats, actual)\n",
    "\n",
    "plot_hist(\n",
    "    test_stats,\n",
    "    actual,\n",
    "    label='difference in means',\n",
    "    title=f'p value for difference in means between two randomly generated datasets is {p_val:0.2f}'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just keep running it until you get a significant result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see where this is going.  If we play this game over and over (or if many researchers play it in parallel), the false positive rate can be as high as 100%.\n",
    "\n",
    "To see this more clearly, let's simulate 100 researchers playing this game.  I'll take the code we have so far and wrap it in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete global variables\n",
    "del groups, actual, test_stats, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(sample_size: int = 200, niters: int = 1000):\n",
    "    \"\"\"Generate random data and run a hypothesis test on it.\n",
    "\n",
    "    sample_size: integer\n",
    "\n",
    "    returns: p-value\n",
    "    \"\"\"\n",
    "    groups = GroupPair(\n",
    "        np.random.normal(100, 15, size=sample_size),\n",
    "        np.random.normal(100, 15, size=sample_size)\n",
    "    )\n",
    "    actual = mean_diff(groups)\n",
    "    return p_value(\n",
    "        run_model(groups, mean_diff, pooled_sample, niters=niters),\n",
    "        actual\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run it 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = np.array([run_experiment(sample_size=500, niters=100) for _ in range(100)])\n",
    "sum(p_vals < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, we expect to get a false positive about 5 times out of 100.  To see why, let's plot the histogram of the p-values we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    p_vals,\n",
    "    0.05,\n",
    "    label='p-value',\n",
    "    title='A sample of p-values obtained through muliple iterations of the same experiment',\n",
    "    # increments of 0.05\n",
    "    bins=np.linspace(0, 1, 21)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of p-values is uniform from 0 to 1.  So it falls below 5% about 5% of the time.\n",
    "\n",
    "If the threshold for statistical signficance is 5%, the probability of a false positive is 5%.  You might hope that things would get better with larger sample sizes, but they don't.  Run this experiment again with a larger sample size, and see for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test of pregnancy length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PregLengthTest:\n",
    "\n",
    "    def __init__(self, gp: GroupPair):\n",
    "        # firsts, others = gp.group1, gp.group2\n",
    "        self.n = len(gp.group1)\n",
    "        pmf = Pmf.from_seq(\n",
    "            np.hstack((gp.group1, gp.group2))\n",
    "        )\n",
    "        self.values = np.arange(35, 44)\n",
    "        self.expected_probs = np.array([pmf.prob(value) for value in self.values])\n",
    "    \n",
    "    def test_statistic(self, gp: GroupPair):\n",
    "        # firsts, others\n",
    "        stat = self.chi_squared(gp.group1) + self.chi_squared(gp.group2)\n",
    "        return stat\n",
    "\n",
    "    def chi_squared(self, lengths):\n",
    "        hist = Counter(lengths)\n",
    "        observed = np.array([hist[value] for value in self.values])\n",
    "        # turn expected probabilities into expected frequencies\n",
    "        expected = self.expected_probs * len(lengths)\n",
    "        return sum((observed - expected)**2 / expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we specifically test the deviations of first babies and others from the expected number of births in each week of pregnancy, the results are statistically significant with a very small p-value.  But at this point we have run so many tests, we should not be surprised to find at least one that seems significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = GroupPair(\n",
    "    firsts.values,\n",
    "    others.values\n",
    ")\n",
    "ht = PregLengthTest(groups)\n",
    "actual = ht.test_statistic(groups)\n",
    "print(f'actual: {actual}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = run_model(groups, ht.test_statistic, pooled_sample, niters=1000)\n",
    "p_val = p_value(test_stats, actual)\n",
    "print(f'p-value: {p_val}')\n",
    "print(f'ts max: {np.max(test_stats)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Power\n",
    "\n",
    "In the previous section, we computed the false positive rate, which is the probability of seeing a \"statistically significant\" result, even if there is no statistical difference between groups.\n",
    "\n",
    "Now let's ask the complementary question: if there really is a difference between groups, what is the chance of seeing a \"statistically significant\" result?\n",
    "\n",
    "The answer to this question is called the \"power\" of the test.  It depends on the sample size (unlike the false positive rate), and it also depends on how big the actual difference is.\n",
    "\n",
    "We can estimate the power of a test by running simulations similar to the ones in the previous section.  Here's a version that takes the actual difference between groups as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment_with_difference(actual_diff: int, sample_size=100, niters = 200) -> float:\n",
    "    \"\"\"\n",
    "    Generate random data and run a hypothesis test on it.\n",
    "\n",
    "    actual_diff: The actual difference between groups.\n",
    "    sample_size: integer\n",
    "\n",
    "    returns: p-value\n",
    "    \"\"\"\n",
    "    groups = GroupPair(\n",
    "        # two groups with different means\n",
    "        np.random.normal(100, 15, size=sample_size),\n",
    "        np.random.normal(100 + actual_diff, 15, size=sample_size)\n",
    "    )\n",
    "    actual = mean_diff(groups)\n",
    "    return p_value(\n",
    "        run_model(groups, mean_diff, pooled_sample, niters=niters),\n",
    "        actual\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it 100 times with an actual difference of 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_values = np.array([run_experiment_with_difference(5) for i in range(100)])\n",
    "sum(p_values < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sample size 100 and an actual difference of 5, the power of the test is approximately 65%.  That means if we ran this hypothetical experiment 100 times, we'd expect a statistically significant result about 65 times.\n",
    "\n",
    "That's pretty good, but it also means we would NOT get a statistically significant result about 35 times, which is a lot.\n",
    "\n",
    "Again, let's look at the distribution of p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    p_values,\n",
    "    0.05,\n",
    "    label='p-value',\n",
    "    title='A sample of p-values with an effect size of 5',\n",
    "    bins=np.linspace(0, 1, 21)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power\n",
    "\n",
    "Here's the function that estimates the probability of a non-significant p-value even is there really is a difference between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(xs: np.array) -> np.array:\n",
    "    return np.random.choice(xs, len(xs), replace=True)\n",
    "\n",
    "def group_resampler(data: GroupPair) -> GroupPair:\n",
    "    # return as a new grouped pair with resampled groups\n",
    "    return GroupPair(resample(data.group1), resample(data.group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GroupPair(firsts, others)\n",
    "actual = mean_diff(data)\n",
    "# actual difference is 0.078 weeks\n",
    "print(f'actual: {actual:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = run_model(data, mean_diff, group_resampler, niters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p-value: {p_value(test_stats, actual)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative_rate(data: GroupPair, num_runs=100):\n",
    "    \"\"\"Computes the chance of a false negative based on resampling.\n",
    "\n",
    "    data: pair of sequences\n",
    "    num_runs: how many experiments to simulate\n",
    "\n",
    "    returns: float false negative rate\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    actual = mean_diff(data)\n",
    "    p_values = np.array([\n",
    "        run_model(data, mean_diff, group_resampler, niters=100) for _ in range(num_runs)\n",
    "    ])\n",
    "    # if the p-value is greater than 0.05 then we accept the null hypothesis\n",
    "    return np.sum(p_values > 0.05) / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_rate(data, num_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the false negative rate is 70%, which means that the power of the test (probability of statistical significance if the actual difference is 0.078 weeks) is only 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the point of this example: if you get a negative result (no statistical significance), that is not always strong evidence that there is no difference between the groups.  It is also possible that the power of the test was too low; that is, that it was unlikely to produce a positive result, even if there is a difference between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** As sample size increases, the power of a hypothesis test increases, which means it is more likely to be positive if the effect is real. Conversely, as sample size decreases, the test is less likely to be positive even if the effect is real.\n",
    "\n",
    "To investigate this behavior, run the tests in this chapter with different subsets of the NSFG data.\n",
    "\n",
    "What happens to the p-values of these tests as sample size decreases? What is the smallest sample size that yields a positive test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have no null values in our features of interest\n",
    "live.loc[:, ['prglngth', 'totalwgt_lb']].apply(lambda col: sum(col.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeatedly divide a starting value until you can divide no more\n",
    "def subdivide(n, by=2, lower_limit = 1) -> List[int]:\n",
    "    vals = []\n",
    "    while n > lower_limit:\n",
    "        vals.append(n)\n",
    "        n //= 2\n",
    "    if vals[-1] > lower_limit:\n",
    "        vals.append(lower_limit)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivide(len(live))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivide(len(live), lower_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rows(df: pd.DataFrame, nrows: int, replace=False) -> pd.DataFrame:\n",
    "    \"\"\"Choose a sample of rows from a DataFrame.\n",
    "\n",
    "    df: DataFrame\n",
    "    nrows: number of rows\n",
    "    replace: whether to sample with replacement\n",
    "\n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    return df.loc[np.random.choice(df.index, nrows, replace=replace)]\n",
    "\n",
    "\n",
    "def resample_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Resamples rows from a DataFrame.\n",
    "\n",
    "    df: DataFrame\n",
    "\n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    return sample_rows(df, len(df), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def run_tests(live: pd.DataFrame, iters=1000):\n",
    "    \"\"\"Runs the tests from Chapter 9 with a subset of the data.\n",
    "\n",
    "    live: DataFrame\n",
    "    iters: how many iterations to run\n",
    "    \"\"\"\n",
    "    n = len(live)\n",
    "    # indicator vectors\n",
    "    firsts = live.birthcat == 'firsts'\n",
    "    others = live.birthcat == 'others'\n",
    "    pvals = []\n",
    "    # test differences of means\n",
    "    # TODO: test for correlation and chi-square\n",
    "    for col in('prglngth', 'totalwgt_lb'):\n",
    "        values = live[col].values\n",
    "        data = GroupPair(values[firsts], values[others])\n",
    "        estimates = run_model(\n",
    "            data,\n",
    "            mean_diff,\n",
    "            pooled_sample,\n",
    "            niters = iters\n",
    "        )\n",
    "        pvals.append(p_value(estimates, mean_diff(data)))\n",
    "    return pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "sample_sizes = subdivide(len(live), lower_limit=50)\n",
    "for n in sample_sizes:\n",
    "    pvals.append(run_tests(sample_rows(live, n), iters=100))\n",
    "pd.DataFrame(\n",
    "    np.vstack(pvals),\n",
    "    index=sample_sizes,\n",
    "    columns=['diff_mean_pregancy_length', 'diff_mean_birth_weight']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: As expected, tests that are positive with large sample sizes become negative as we take away data.  But the pattern is erratic, with some positive tests even at small sample sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
