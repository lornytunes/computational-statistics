{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compstats\n",
    "import nsfg\n",
    "import hypothesis\n",
    "\n",
    "from cdf import Cdf\n",
    "from normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "sns.set_theme()\n",
    "figsize(9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = partial(np.round, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distributions\n",
    "\n",
    "As a motivating example, let’s review the problem from [Estimation](08_Estimation.ipynb)\n",
    "\n",
    "Suppose you are a scientist studying gorillas in a wildlife preserve. Having weighed 9 gorillas, you find sample mean $\\bar{x} = 90kg$ and sample standard deviation, $S = 7.5 kg$. If you use $\\bar{x}$ to estimate the population mean, what is the standard error of the estimate?\n",
    "\n",
    "If we know the parameters of the sampling distribution, we can compute confidence intervals and p-values analytically, which is computationally faster than resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def qnorm(p, mu=0, sigma=1):\n",
    "    return stats.norm.ppf(p, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the confidence interval for the estimated mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qnorm((0.05, 0.95), mu=90, sigma=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normal.py` provides a `Normal` class that encapsulates what we know about arithmetic operations on normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dist = Normal(90, 7.5**2)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to compute the sampling distribution of the mean with sample size 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dist_xbar = dist.sum(9) / 9\n",
    "dist_xbar.sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then compute a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_xbar.percentile((0.05, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "As we saw in the previous sections, if we add values drawn from normal distributions, the distribution of the sum is normal. Most other distributions don’t have this property; if we add values drawn from other distributions, the sum does not generally have an analytic distribution.\n",
    "\n",
    "But if we add up n values from almost any distribution, the distribution of the sum converges to normal as n increases.\n",
    "\n",
    "More specifically, if the distribution of the values has mean and standard deviation μ and σ, the distribution of the sum is approximately $\\mathcal{N}(n \\mu, n \\sigma^2)$\n",
    "This result is the Central Limit Theorem (CLT). It is one of the most useful tools for statistical analysis, but it comes with caveats:\n",
    "\n",
    "- The values have to be drawn independently. If they are correlated, the CLT doesn’t apply (although this is seldom a problem in practice).\n",
    "- The values have to come from the same distribution (although this requirement can be relaxed).\n",
    "- The values have to be drawn from a distribution with finite mean and variance. So most Pareto distributions are out.\n",
    "- The rate of convergence depends on the skewness of the distribution. Sums from an exponential distribution converge for small n. Sums from a lognormal distribution require larger sizes.\n",
    "\n",
    "The Central Limit Theorem explains the prevalence of normal distributions in the natural world. Many characteristics of living things are affected by genetic and environmental factors whose effect is additive. The characteristics we measure are the sum of a large number of small effects, so their distribution tends to be normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CLT\n",
    "\n",
    "The following function generates samples with difference sizes from an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_expo_sample(sample_size: int, beta=2.0, iters=1000):\n",
    "    \"\"\"Generates samples from an exponential distribution.\n",
    "\n",
    "    beta: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: a list of the sums of each sample\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        np.sum(np.random.exponential(beta, sample_size)) for _ in range(iters)\n",
    "    ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 sums of random exponential samples of size 1000\n",
    "samples = make_expo_sample(100, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean should be close to 100*2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(samples.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the values should be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.ecdfplot(\n",
    "    x = samples\n",
    ");\n",
    "p.set(\n",
    "    xlabel = 'Sums',\n",
    "    ylabel = 'CDF',\n",
    "    title = 'CDF of 1000 sums of exponential samples of size 100'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_qq_plot(values: np.array, label: str, title: str, legend=True, trim=True):\n",
    "    \"\"\"Generates a normal probability plot.\n",
    "\n",
    "    weights: sequence\n",
    "    \"\"\"\n",
    "    if trim:\n",
    "        mu, var = compstats.trimmed_mean_var(values, p=0.01)\n",
    "    else:\n",
    "        mu, var = values.mean(), values.var()\n",
    "    std = np.sqrt(var)\n",
    "    xs, ys = compstats.normal_qq(values)\n",
    "    xlims = (-5, 5)\n",
    "    fitted_ys = compstats.fit_line(np.array(xlims), mu, std)\n",
    "    plt.plot(xs, ys, label = 'data')\n",
    "    plt.plot(xlims, fitted_ys, label='fitted', linestyle='dashed')\n",
    "    plt.xlabel('z')\n",
    "    plt.xlim(xlims)\n",
    "    plt.ylabel(label)\n",
    "    plt.title(title)\n",
    "    if legend:\n",
    "        plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_qq_plot(samples, label = 'Sum of exponental values', title='Sample size = 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the convergence given the sample size n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [1, 10, 100]\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(sample_sizes),\n",
    "    figsize=(13, 5,)\n",
    ")\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    samples = make_expo_sample(sample_size)\n",
    "    p = sns.ecdfplot(\n",
    "        x=samples,\n",
    "        ax=axs[i]\n",
    "    )\n",
    "    p.set(\n",
    "        xlabel = 'Sums',\n",
    "        ylabel = 'CDF',\n",
    "        title = f'Sample size = {sample_size}'\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows how the sum of exponential variates converges to normal as sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize(13, 5)\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(sample_sizes), figsize=(13, 5))\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    plt.subplot(1, len(sample_sizes), i+1)\n",
    "    normal_qq_plot(\n",
    "        values = make_expo_sample(sample_size),\n",
    "        label = '',\n",
    "        title = f'Sample size = {sample_size}',\n",
    "        legend=False\n",
    "    )\n",
    "fig.suptitle('Sums of exponential values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_qq_plots(sample_sizes: List[int], sampler: Callable, title: str, trim=True):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(sample_sizes), figsize=(13, 5))\n",
    "    for i, sample_size in enumerate(sample_sizes):\n",
    "        plt.subplot(1, len(sample_sizes), i+1)\n",
    "        normal_qq_plot(\n",
    "            values = sampler(sample_size),\n",
    "            label = '',\n",
    "            title = f'Sample size = {sample_size}',\n",
    "            legend=False,\n",
    "            trim=trim\n",
    "        )\n",
    "    fig.suptitle(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qq_plots(sample_sizes, make_expo_sample, 'Sums of exponential values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal distribution has higher variance, so it requires a larger sample size before it converges to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_lognormal_sample(sample_size: int, mu=1.0, sigma=1.0, iters=1000):\n",
    "    \"\"\"Generates samples from a lognormal distribution.\n",
    "\n",
    "    mu: parmeter\n",
    "    sigma: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(np.random.lognormal(mu, sigma, sample_size)) for _ in range(iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qq_plots(sample_sizes, make_lognormal_sample, 'Sums of lognormal values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pareto distribution has infinite variance, and sometimes infinite mean, depending on the parameters.  It violates the requirements of the CLT and does not generally converge to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_pareto_sample(sample_size: int, alpha=1.0, iters=1000):\n",
    "    \"\"\"Generates samples from a Pareto distribution.\n",
    "\n",
    "    alpha: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(np.random.pareto(alpha, sample_size)) for _ in range(iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qq_plots(sample_sizes, make_pareto_sample, 'Sums of pareto values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the random variates are correlated, that also violates the CLT, so the sums don't generally converge.\n",
    "\n",
    "To generate correlated values, we generate correlated normal values and then transform to whatever distribution we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_correlated(rho, n):\n",
    "    \"\"\"Generates a sequence of correlated values from a standard normal dist.\n",
    "    \n",
    "    rho: coefficient of correlation\n",
    "    n: length of sequence\n",
    "\n",
    "    returns: iterator\n",
    "    \"\"\"\n",
    "    x = random.gauss(0, 1)\n",
    "    yield x\n",
    "    # each subsequent values depends on its predecessor\n",
    "    sigma = np.sqrt(1 - rho**2)\n",
    "    for _ in range(n-1):\n",
    "        # takes the standard deviation as its second argument, not the variance\n",
    "        x = random.gauss(x * rho, sigma)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_expo_correlated(rho: float, n: int):\n",
    "    \"\"\"Generates a sequence of correlated values from an exponential dist.\n",
    "\n",
    "    rho: coefficient of correlation\n",
    "    n: length of sequence\n",
    "\n",
    "    returns: NumPy array\n",
    "    \"\"\"\n",
    "    # generate correlated normal values\n",
    "    normal = list(generate_correlated(rho, n))\n",
    "    # use the normal CDF to transform the values to uniform\n",
    "    uniform = stats.norm.cdf(normal)\n",
    "    # inverse exponential CDF to transform the uniform values to exponential\n",
    "    expo = stats.expon.ppf(uniform)\n",
    "    return expo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correlated_sample(sample_size: int, rho=0.9, iters=1000):\n",
    "    \"\"\"Generates samples from a correlated exponential distribution.\n",
    "\n",
    "    rho: correlation\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"    \n",
    "    \n",
    "    return np.array(\n",
    "        [np.sum(generate_expo_correlated(rho, sample_size)) for _ in range(iters)]\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qq_plots(\n",
    "    sample_sizes,\n",
    "    partial(make_correlated_sample, rho=0.9, iters=1000),\n",
    "    'Sum of correlated exponential values'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the CLT\n",
    "\n",
    "Let's use analytic methods to compute a CI and p-value for an observed difference in means.\n",
    "\n",
    "The distribution of pregnancy length is not normal, but it has finite mean and variance, so the sum (or mean) of a few thousand samples is very close to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live = nsfg.read_live_fem_preg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts = live.birthcat == 'firsts'\n",
    "others = live.birthcat == 'others'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why the Central Limit Theorem is useful, let’s get back to the example in Section 9.3: testing the apparent difference in mean pregnancy length for first babies and others. As we’ve seen, the apparent difference is about 0.078 weeks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.abs(np.diff(live.groupby('birthcat')['prglngth'].mean()).item())\n",
    "np.round(delta, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the sampling distribution of the mean for a set of values and a given sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sampling_dist_mean(data: np.array, n: int) -> Normal:\n",
    "    \"\"\"Computes the sampling distribution of the mean.\n",
    "\n",
    "    data: sequence of values representing the population\n",
    "    n: sample size\n",
    "\n",
    "    returns: Normal object\n",
    "    \"\"\"\n",
    "    mean, var = data.mean(), data.var()\n",
    "    dist = Normal(mean, var)\n",
    "    return dist.sum(n) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the sampling distributions for the means of the two groups under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = sampling_dist_mean(live.prglngth, np.sum(firsts))\n",
    "dist2 = sampling_dist_mean(live.prglngth, np.sum(others))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the sampling distribution for the difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dist1 - dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis, here's the chance of exceeding the observed difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(1 - dist.prob(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the chance of falling below the negated difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(dist.prob(-delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of these probabilities is the two-sided p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(2 * dist.prob(-delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is consistent with the estimate in [Hypothesis Testing](09_Hypothesis_Testing.ipynb), which was 0.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a correlation\n",
    "\n",
    "Under the null hypothesis (that there is no correlation), the sampling distribution of the observed correlation (suitably transformed) is a \"Student t\" distribution.\n",
    "\n",
    "The method is based on this mathematical result: given two variables that are normally distributed and uncorrelated, if we generate a sample with size n, compute Pearson’s correlation, r, and then compute the transformed correlation\n",
    "\n",
    "$$\n",
    "t = r \\sqrt{\\frac{n-2}{1-r^2}}\n",
    "$$\n",
    "\n",
    "the distribution of t is Student’s t-distribution with parameter n − 2. The t-distribution is an analytic distribution; the CDF can be computed efficiently using gamma functions.\n",
    "\n",
    "We can use this result to compute the sampling distribution of correlation under the null hypothesis; that is, if we generate uncorrelated sequences of normal values, what is the distribution of their correlation? `student_cdf` takes the sample size, n, and returns the sampling distribution of correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_t(r: float, n: int) -> float:\n",
    "    return r * np.sqrt((n-2) / (1-r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get from ts to the correlation coefficients, rs, we apply the inverse transform,\n",
    "\n",
    "$$\n",
    "r = \\frac{t}{\\sqrt{n-2-t^2}}\n",
    "$$\n",
    "\n",
    "The result is the sampling distribution of r under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def student_cdf(n: int) -> Cdf:\n",
    "    \"\"\"Computes the CDF correlations from uncorrelated variables.\n",
    "\n",
    "    n: sample size\n",
    "\n",
    "    returns: Cdf\n",
    "    \"\"\"\n",
    "    ts = np.linspace(-3, 3, 101)\n",
    "    ps = stats.t.cdf(ts, df=n-2)\n",
    "    rs = ts / np.sqrt(n - 2 + ts**2)\n",
    "    return Cdf(rs, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a `HypothesisTest` that uses permutation to estimate the sampling distribution of a correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_test_stat(gp: hypothesis.GroupPair) -> np.float64:\n",
    "    return np.corrcoef(gp.group1, gp.group2)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hypothesis.GroupPair(\n",
    "    live.agepreg.values,\n",
    "    live.totalwgt_lb.values\n",
    ")\n",
    "actual = cor_test_stat(data)\n",
    "test_stats = hypothesis.run_model(\n",
    "    data,\n",
    "    cor_test_stat,\n",
    "    hypothesis.permutation_sampler,\n",
    "    niters=10000\n",
    ")\n",
    "p_val = hypothesis.p_value(test_stats, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf of the null hypothesis (no-correlation)\n",
    "model_cdf = student_cdf(len(live))\n",
    "# our sample distribution\n",
    "sample_cdf = Cdf.from_seq(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can estimate the sampling distribution by permutation and compare it to the Student t distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(8, 6)\n",
    "plt.plot(\n",
    "    model_cdf.xs,\n",
    "    model_cdf.ps,\n",
    "    color='darkred',\n",
    "    alpha=0.5,\n",
    "    label='Student t'\n",
    ")\n",
    "sns.ecdfplot(\n",
    "    x=test_stats,\n",
    "    alpha=0.5,\n",
    "    label='sample'\n",
    ")\n",
    "# plt.plot(\n",
    "#     sample_cdf.xs,\n",
    "#     sample_cdf.ps,\n",
    "#     color='royalblue',\n",
    "#     alpha=0.5,\n",
    "#     label='sample'\n",
    "# )\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are nearly identical. Although the actual distributions are not normal, Pearson’s coefficient of correlation is based on sample means and variances. By the Central Limit Theorem, these moment- based statistics are normally distributed even if the data are not.\n",
    "\n",
    "From the above figure, we can see that the observed correlation, 0.07, is unlikely to occur if the variables are actually uncorrelated. Using the analytic distri- bution, we can compute just how unlikely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = len(live)\n",
    "# compute the t that corresponds to r=0.07 (r is actual)\n",
    "t_stat = compute_t(actual, n)\n",
    "# and evaluate the t distribution at t\n",
    "p_val = 1 - stats.t.cdf(t_stat, df=n-2)\n",
    "print(f'r: {actual:0.2f}, t: {t_stat:0.2f}, pval: {p_val:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates an advantage of the analytic method: we can compute very small p-values. But in practice it usually doesn’t matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chi-squared test\n",
    "\n",
    "The reason the chi-squared statistic is useful is that we can compute its distribution under the null hypothesis analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chi_squared_cdf(n):\n",
    "    \"\"\"Discrete approximation of the chi-squared CDF with df=n-1.\n",
    "\n",
    "    n: sample size\n",
    "    \n",
    "    returns: Cdf\n",
    "    \"\"\"\n",
    "    xs = np.linspace(0, 25, 101)\n",
    "    ps = stats.chi2.cdf(xs, df=n-1)\n",
    "    return Cdf(xs, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can confirm the analytic result by comparing values generated by simulation with the analytic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [8, 9, 19, 5, 8, 11]\n",
    "dt = hypothesis.DiceChiTest(data)\n",
    "p_value = dt.p_value(iters=1000)\n",
    "print(f'pval: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "model_cdf = chi_squared_cdf(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    model_cdf.xs,\n",
    "    model_cdf.ps,\n",
    "    color='darkred',\n",
    "    alpha=0.5,\n",
    "    label='Chi squared'\n",
    ")\n",
    "sns.ecdfplot(\n",
    "    x=dt.test_stats,\n",
    "    alpha=0.5,\n",
    "    label='sample'\n",
    ")\n",
    "plt.xlabel('chi-squared statistic')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can use the analytic distribution to compute p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "chi2 = dt.actual\n",
    "p_val = 1 - stats.chi2.cdf(chi2, df=n-1)\n",
    "print(f'chi2: {chi2}, p-value: {p_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**    In Section 5.4, we saw that the distribution of adult weights is approximately lognormal. One possible explanation is that the weight a person gains each year is proportional to their current weight. In that case, adult weight is the product of a large number of multiplicative factors:\n",
    "\n",
    "$$\n",
    "w = w_0 f_1 f_2 \\cdots f_n  \n",
    "$$\n",
    "\n",
    "where w is adult weight, $w_0$ is birth weight, and $f_i$ is the weight gain factor for year i.\n",
    "\n",
    "The log of a product is the sum of the logs of the factors:\n",
    "\n",
    "$$\n",
    "log(w) = log(w_0) + log(f_1) + log(f_2) + \\cdots + log(f_n) \n",
    "$$\n",
    "\n",
    "So by the Central Limit Theorem, the distribution of log(w) is approximately normal for large n, which implies that the distribution of w is lognormal.\n",
    "\n",
    "To model this phenomenon, choose a distribution for f that seems reasonable, then generate a sample of adult weights by choosing a random value from the distribution of birth weights, choosing a sequence of factors from the distribution of f, and computing the product. What value of n is needed to converge to a lognormal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_adult_weight(birth_weights: np.array, n: int):\n",
    "    \"\"\"Generate a random adult weight by simulating annual gain.\n",
    "\n",
    "    birth_weights: sequence of birth weights in lbs\n",
    "    n: number of years to simulate\n",
    "\n",
    "    returns: adult weight in lbs\n",
    "    \"\"\"\n",
    "    bw = random.choice(birth_weights)\n",
    "    factors = np.random.normal(1.09, 0.03, n)\n",
    "    aw = bw * np.prod(factors)\n",
    "    return aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_weights = live.totalwgt_lb.values\n",
    "aws = [generate_adult_weight(birth_weights, 40) for _ in range(1000)]\n",
    "log_aws = np.log10(aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_qq_plot(\n",
    "    log_aws,\n",
    "    label = 'adult weight (log10 lbs)',\n",
    "    title='Log normal weight distribution after 40 years'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. With n=40 the distribution is approximately lognormal except for the lowest weights.\n",
    "1. Actual distribution might deviate from lognormal because it is a mixture of people at different ages, or because annual weight gains are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In Section 14.6 we used the Central Limit Theorem to find the sampling distribution of the difference in means, δ, under the null hypothesis that both samples are drawn from the same population.\n",
    "\n",
    "We can also use this distribution to find the standard error of the estimate and confidence intervals, but that would only be approximately correct. To be more precise, we should compute the sampling distribution of δ under the alternate hypothesis that the samples are drawn from different populations.\n",
    "\n",
    "Compute this distribution and use it to calculate the standard error and a 90% confidence interval for the difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts_mean, others_mean = live.groupby('birthcat')['prglngth'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the observed difference in means\n",
    "delta = firsts_mean - others_mean\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis, both sampling distributions are based on all live births."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = sampling_dist_mean(live.prglngth, firsts.sum())\n",
    "dist2 = sampling_dist_mean(live.prglngth, others.sum())\n",
    "dist_diff_null = dist1 - dist2\n",
    "print(f'null hypothesis: {dist_diff_null}')\n",
    "print(f'{dist_diff_null.prob(-delta):0.4f}, {1 - dist_diff_null.prob(delta):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the alternate hypothesis, each sampling distribution is based on the observed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = sampling_dist_mean(live.prglngth[firsts].values, len(firsts))\n",
    "dist2 = sampling_dist_mean(live.prglngth[others].values, len(others))\n",
    "dist_diff_alt = dist1 - dist2\n",
    "print(f'estimated params: {dist_diff_alt}')\n",
    "print(f'{dist_diff_alt.percentile(0.05):0.4f}, {1 - dist_diff_alt.percentile(0.95):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_dist(dist: Normal, label=None):\n",
    "    xs, ys = dist.render()\n",
    "    opts = {}\n",
    "    if label:\n",
    "        opts['label'] = label\n",
    "    sns.lineplot(\n",
    "        x = xs,\n",
    "        y = ys,\n",
    "        **opts\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_dist(dist_diff_null, label='null hypothesis');\n",
    "plot_normal_dist(dist_diff_alt, label = 'estimated params');\n",
    "plt.xlabel('Difference in means (weeks');\n",
    "plt.ylabel('CDF');\n",
    "plt.xlim([-0.20, 0.25]);\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** [In a recent paper](http://ieeexplore.ieee.org/document/7044435/), Stein et al. investigate the effects of an intervention intended to mitigate gender-stereotypical task allocation within student engineering teams.\n",
    "\n",
    "Before and after the intervention, students responded to a survey that asked them to rate their contribution to each aspect of class projects on a 7-point scale.\n",
    "\n",
    "Before the intervention, male students reported higher scores for the programming aspect of the project than female students; on average men reported a score of 3.57 with standard error 0.28. Women reported 1.91, on average, with standard error 0.32.\n",
    "\n",
    "Compute the sampling distribution of the gender gap (the difference in means), and test whether it is statistically significant. Because you are given standard errors for the estimated means, you don’t need to know the sample size to figure out the sampling distributions.\n",
    "\n",
    "After the intervention, the gender gap was smaller: the average score for men was 3.44 (SE 0.16); the average score for women was 3.18 (SE 0.16). Again, compute the sampling distribution of the gender gap and test it.\n",
    "\n",
    "Finally, estimate the change in gender gap; what is the sampling distribution of this change, and is it statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(dist: Normal, invert=True):\n",
    "    pval = dist.prob(0)\n",
    "    if invert:\n",
    "        pval = 1 - pval\n",
    "    print(f'mean   : {dist.mu:0.2f}, p-value: {pval:0.4f}')\n",
    "    print(f'CI     : ({dist.percentile(0.05):0.2f}, {dist.percentile(0.95):0.2f})')\n",
    "    print(f'stderr : {dist.sigma:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# these are sampling distributions\n",
    "male_before = Normal(3.57, 0.28**2)\n",
    "male_after = Normal(3.44, 0.16**2)\n",
    "\n",
    "female_before = Normal(1.91, 0.32**2)\n",
    "female_after = Normal(3.18, 0.16**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before\n",
    "diff_before = female_before - male_before\n",
    "plot_dist(diff_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# after\n",
    "diff_after = female_after - male_after\n",
    "plot_dist(diff_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference\n",
    "diff = diff_after - diff_before\n",
    "plot_dist(diff, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "1. Gender gap before intervention was 1.66 points (p-value 5e-5)\n",
    "1. Genger gap after was 0.26 points (p-value 0.13, not significant)\n",
    "1. Change in gender gap was 1.4 points (p-value 0.002, significant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
