{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsfg\n",
    "import brfss\n",
    "import compstats\n",
    "import hypothesis\n",
    "from cdf import Cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "sns.set_theme()\n",
    "figsize(11, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load up the NSFG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live = nsfg.read_live_fem_preg().loc[:, [\n",
    "    'agepreg',\n",
    "    'totalwgt_lb',\n",
    "    'finalwgt',\n",
    "    'birthcat'\n",
    "]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live.apply(lambda col: col.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the intercept and slope of the least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(xs: np.array, ys: np.array) -> Tuple[float, float]:\n",
    "    meanx = xs.mean()\n",
    "    meany = ys.mean()\n",
    "    slope = compstats.cov(xs, ys, meanx, meany) / xs.var()\n",
    "    intercept = meany - (slope * meanx)\n",
    "\n",
    "    return intercept, slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the least squares fit to birth weight as a function of mother's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = live.agepreg.values\n",
    "weights = live.totalwgt_lb.values\n",
    "intercept, slope = least_squares(ages, weights)\n",
    "print(f'Intercept: {intercept:0.2f}, Slope: {slope:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept is often easier to interpret if we evaluate it at the mean of the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(intercept + slope * ages.mean(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the mean age is about 25 years and the mean baby weight for a 25 year old mother is 7.3 pounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the slope is easier to interpret if we express it in pounds per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(slope * 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope is 0.27 ounces per year, or 0.17 pounds per decade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates the fitted line at the given `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(xs, intercept, slope) -> Tuple[np.array, np.array]:\n",
    "    fit_xs = np.sort(xs)\n",
    "    fit_ys = intercept + slope * fit_xs\n",
    "    return fit_xs, fit_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a scatterplot of the data with the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.scatterplot(\n",
    "    x = ages,\n",
    "    y = weights,\n",
    "    color = 'royalblue',\n",
    "    alpha=0.1,\n",
    "    s=10\n",
    ");\n",
    "p.axline((10, intercept + 10*slope), slope=slope, color='darkred')\n",
    "p.set(\n",
    "    xlabel = 'Mothers age (in years)',\n",
    "    ylabel = 'Birth weight (lbs)',\n",
    "    ylim = (0, 15)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "The following functon computes the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(xs: np.array, ys: np.array, intercept: float, slope: float) -> np.array:\n",
    "    # actual - predicted\n",
    "    return ys - (intercept + slope * xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the residuals as a column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live['residual'] = residuals(ages, weights, intercept, slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the residuals, I'll split the respondents into groups by age, then plot the percentiles of the residuals versus the average age in each group.\n",
    "\n",
    "First I'll make the groups and compute the average age in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(10, 48, 3)\n",
    "indices = np.digitize(live.agepreg, bins)\n",
    "groups = live.groupby(indices)\n",
    "\n",
    "age_means = [group.agepreg.mean() for _, group in groups][1:-1]\n",
    "age_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll compute the CDF of the residuals in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfs = [Cdf.from_seq(group.residual) for _, group in groups][1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots percentiles of the residuals against the average age in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentiles(age_means: List[float], cdfs: List[Cdf]):\n",
    "    for percent in [75, 50, 25]:\n",
    "        weight_percentiles = [cdf.percentile(percent) for cdf in cdfs]\n",
    "        label = f'{percent}th'\n",
    "        plt.plot(age_means, weight_percentiles, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the 25th, 50th, and 75th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percentiles(age_means, cdfs)\n",
    "\n",
    "plt.xlabel(\"Mother's age (years)\")\n",
    "plt.ylabel(\"Residual (lbs)\")\n",
    "plt.xlim([10, 45]);\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is near zero, as expected, and the interquartile range is about 2 pounds. So if we know the mother’s age, we can guess the baby’s weight within a pound, about 50% of the time\n",
    "\n",
    "Ideally these lines should be flat, indicating that the residuals are random, and parallel, indicating that the variance of the residuals is the same for all age groups. In fact, the lines are close to parallel, so that’s good; but they have some curvature, indicating that the relationship is nonlinear. Nevertheless, the linear fit is a simple model that is probably good enough for some purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "The parameters slope and inter are estimates based on a sample; like other estimates, they are vulnerable to sampling bias, measurement error, and sampling error. As discussed in [chaper 8](08_Estimation.ipynb), sampling bias is caused by non-representative sampling, measurement error is caused by errors in collecting and recording data, and sampling error is the result of measuring a sample rather than the entire population.\n",
    "\n",
    "To assess sampling error, we ask, “If we run this experiment again, how much variability do we expect in the estimates?” We can answer this question by running simulated experiments and computing sampling distributions of the estimates.\n",
    "\n",
    "To estimate the sampling distribution of `inter` and `slope`, I'll use resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simulate the experiments by resampling the data; that is, I treat the observed pregnancies as if they were the entire population and draw samples, with replacement, from the observed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_distibutions(live: pd.DataFrame, iters=1001) -> Tuple[np.array, np.array]:\n",
    "    t = []\n",
    "    for _ in range(iters):\n",
    "        sample = compstats.resample_rows(live)\n",
    "        # use the simulated sample to estimate parameters\n",
    "        t.append(\n",
    "            least_squares(sample.agepreg, sample.totalwgt_lb)\n",
    "        )\n",
    "    inters, slopes = zip(*t)\n",
    "    # return the estimated intercepts and the estimated slopes\n",
    "    return np.array(inters), np.array(slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inters, slopes = sampling_distibutions(live)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of estimates and prints the mean, standard error, and 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(estimates: np.array, actual=None):\n",
    "    mean = estimates.mean()\n",
    "    stderr = compstats.std(estimates, mu=actual)\n",
    "    # 90 percent confidence interval (between 0.05 and 0.95)\n",
    "    ci = np.percentile(estimates, (5, 95))\n",
    "    print(f'mean {mean:0.2f}, SE: {stderr:0.3f}, CI: {np.round(ci,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's  the summary for `inter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(inters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for `slope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use `resample_rows` and generate a list of estimates for the mean birth weight. Use `summarize` to compute the SE and CI for these estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "estimates = np.array([compstats.resample_rows(live).totalwgt_lb.mean() for _ in range(iters)])\n",
    "summarize(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing uncertainty\n",
    "\n",
    "To show the uncertainty of the estimated slope and intercept, we can generate a fitted line for each resampled estimate and plot them on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slope, inter in zip(slopes, inters):\n",
    "    fxs, fys = fit_line(age_means, inter, slope)\n",
    "    plt.plot(fxs, fys, color='darkgray', alpha=0.01)\n",
    "    plt.plot(fxs, fys, color='gray', alpha=0.01)\n",
    "plt.xlabel(\"Mother's age (years)\")\n",
    "plt.ylabel(\"Residual (lbs)\")\n",
    "plt.xlim([10, 45]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can make a neater (and more efficient plot) by computing fitted lines and finding percentiles of the fits for each value of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_intervals(xs, inters, slopes, percent=90, **options):\n",
    "    fys_seq = []\n",
    "    for inter, slope in zip(inters, slopes):\n",
    "        fxs, fys = fit_line(xs, inter, slope)\n",
    "        fys_seq.append(fys)\n",
    "\n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100 - p\n",
    "    low, high = compstats.percentile_rows(fys_seq, percents)\n",
    "    plt.fill_between(\n",
    "        fxs, low, high, **options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the confidence interval for the fitted values at each mother's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_intervals(\n",
    "    age_means, inters, slopes,\n",
    "    percent=50,\n",
    "    color='gray',\n",
    "    alpha=0.5,\n",
    "    label='50% CI'\n",
    ")\n",
    "\n",
    "plot_confidence_intervals(\n",
    "    age_means, inters, slopes,\n",
    "    percent=90, \n",
    "    color='gray',\n",
    "    alpha=0.3,\n",
    "    label='90% CI'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Mother's age (years)\")\n",
    "plt.ylabel(\"Residual (lbs)\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim([13, 45]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% and 90% confidence intervals showing variability in thefitted line due to sampling error of intercept and slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of fit\n",
    "\n",
    "There are several ways to measure the quality of a linear model, or goodness of fit. One of the simplest is the standard deviation of the residuals.\n",
    "\n",
    "If you use a linear model to make predictions, `std(res)` is the root mean squared error (RMSE) of your predictions. For example, if you use mother’s age to guess birth weight, the RMSE of your guess would be 1.40 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(compstats.std(live.residual), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you guess birth weight without knowing the mother’s age, the RMSE of your guess is `std(ys)`, which is 1.41 lbs. So in this example, knowing a mother’s age does not improve the predictions substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(compstats.std(weights), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to measure goodness of fit is the coefficient of determination, usually denoted $R^2$ and called “R-squared”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_determination(ys: np.array, res: np.array):\n",
    "    # the variance explained by the model (1-res(var)) as a proportion of the total variance\n",
    "    return 1 - res.var() / ys.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`var(res)` is the MSE of your guesses using the model, `var(ys)` is the MSE without it. So their ratio is the fraction of MSE that remains if you use the model, and $R^2$ is the fraction of MSE the model eliminates.\n",
    "\n",
    "For birth weight and mother's age $R^2$ is very small, indicating that the mother's age predicts a small part of the variance in birth weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter, slope = least_squares(ages, weights)\n",
    "res = residuals(ages, weights, inter, slope)\n",
    "r2 = coef_determination(weights, res)\n",
    "np.round(r2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that $R^2 = \\rho^2$ (Pearson’s coefficient of correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'rho: {compstats.corr(ages, weights):0.4f}, R: {np.sqrt(r2):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To express predictive power, I think it's useful to compare the standard deviation of the residuals to the standard deviation of the dependent variable, as a measure RMSE if you try to guess birth weight with and without taking into account mother's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'std(ys): {compstats.std(weights):0.3f}, std(res): {compstats.std(res):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when people talk about the validity of the SAT (a standardized test used for college admission in the U.S.) they often talk about correlations between SAT scores and other measures of intelligence.\n",
    "\n",
    "According to one study, there is a Pearson correlation of $\\rho = 0.72$ between total SAT scores and IQ scores, which sounds like a strong correlation, but $R^2=\\rho^2=0.52$ so SAT scores account for only 52% of the variance in IQ.\n",
    "\n",
    "IQ scores are normalized with `std(ys) = 15`, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ys = 15**2\n",
    "rho = 0.72\n",
    "r2 = rho**2\n",
    "# coefficient of determination\n",
    "var_res = (1 - r2) * var_ys\n",
    "std_res = np.sqrt(var_res)\n",
    "np.round(std_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using SAT score to predict IQ reduces RMSE from 15 points to 10.4 points. A correlation of 0.72 yields a reduction in RMSE of only 31%.\n",
    "\n",
    "If you see a correlation that looks impressive, remember that $R^2$ is a better indicator of reduction in MSE, and reduction in RMSE is a better indicator of predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing with slopes\n",
    "\n",
    "Another approach is to test whether the apparent slope is due to chance. The null hypothesis is that the slope is actually zero; in that case we can model the birth weights as random variations around their mean. Here’s a hypothesis test for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlopeTest:\n",
    "    \n",
    "    def __init__(self, data: hypothesis.GroupPair):\n",
    "        # ages, weights\n",
    "        weights = data.group2\n",
    "        self.ybar = weights.mean()\n",
    "        self.res = weights - self.ybar\n",
    "\n",
    "    def test_statistic(self, data: hypothesis.GroupPair):\n",
    "        # the slope estimated by least squares\n",
    "        _, slope = least_squares(data.group1, data.group2)\n",
    "        return slope\n",
    "\n",
    "    def resample(self, data: hypothesis.GroupPair):\n",
    "        # permute the deviations (residuals) and add them to the mean\n",
    "        weights = self.ybar + np.random.permutation(self.res)\n",
    "        return hypothesis.GroupPair(data.group1, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are represented as sequences of ages and weights. The test statistic is the slope estimated by `least_squares`. The model of the null hypothesis is represented by the mean weight of all babies and the deviations from the mean. To generate simulated data, we permute the deviations and add them to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hypothesis.GroupPair(ages, weights)\n",
    "st = SlopeTest(data)\n",
    "actual = st.test_statistic(data)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = hypothesis.run_model(\n",
    "    data,\n",
    "    test_stat=st.test_statistic,\n",
    "    sampler=st.resample\n",
    ")\n",
    "print(f'actual: {actual:0.2}, \\\n",
    "    p-value {hypothesis.p_value(estimates, actual)}, \\\n",
    "    max: {np.max(estimates):0.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=estimates);\n",
    "plt.axvline(actual, color='darkred', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis, the largest slope we observe after 1000 tries is substantially less than the observed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use resampling to estimate the sampling distribution of the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of slopes under the null hypothesis, and the sampling distribution of the slope under resampling, have the same shape, but one has mean at 0 and the other has mean at the observed slope.\n",
    "\n",
    "To compute a p-value, we can count how often the estimated slope under the null hypothesis exceeds the observed slope, or how often the estimated slope under resampling falls below 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(x=estimates, label='Null hypothesis');\n",
    "sns.ecdfplot(x=slopes, label='Sampling distribution');\n",
    "plt.axvline(estimates.mean(), color='gray', linestyle='--')\n",
    "plt.axvline(slopes.mean(), color='gray', linestyle='--')\n",
    "plt.xlim([-0.03, 0.03])\n",
    "plt.xlabel('slope (lbs / year)')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling distribution of the estimated slope and the distribution of slopes generated under the null hypothesis. The vertical lines are at 0 and the observed slope, 0.017 lbs/year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to get a p-value from the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_cdf = Cdf.from_seq(slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of slopes that match the null distribution (i.e zero or no slope)\n",
    "pvalue = sampling_cdf.prob(0)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted resampling\n",
    "\n",
    "So far we have treated the NSFG data as if it were a representative sample, but as I mentioned in Section 1.2, it is not. The survey deliberately oversamples several groups in order to improve the chance of getting statistically significant results; that is, in order to improve the power of tests involving these groups.\n",
    "\n",
    "This survey design is useful for many purposes, but it means that we cannot use the sample to estimate values for the general population without accounting for the sampling process.\n",
    "\n",
    "For each respondent, the NSFG data includes a variable called finalwgt, which is the number of people in the general population the respondent represents. This value is called a sampling weight.\n",
    "\n",
    "Resampling provides a convenient way to take into account the sampling weights associated with respondents in a stratified survey design.\n",
    "\n",
    "As an example, if you survey 100,000 people in a country of 300 million, each respondent represents 3,000 people. If you oversample one group by a factor of 2, each person in the oversampled group would have a lower weight, about 1500.\n",
    "\n",
    "To correct for oversampling, we can use resampling; that is, we can draw samples from the survey using probabilities proportional to sampling weights. Then, for any quantity we want to estimate, we can generate sampling dis- tributions, standard errors, and confidence intervals. As an example, I will estimate mean birth weight with and without sampling weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we saw `resample_rows`, which chooses rows from a DataFrame, giving each row the same probability. \n",
    "\n",
    "Now we need to do the same thing using probabilities proportional to sampling weights. \n",
    "\n",
    "`resample_rows_weighted` takes a DataFrame, resamples rows according to the weights in `finalwgt`, and returns a DataFrame containing the resampled rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = Cdf.from_dict(dict(live.finalwgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rows appear far more than others\n",
    "pd.Series(cdf.sample(len(live))).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_rows_weighted(df, column='finalwgt'):\n",
    "    weights = df[column]\n",
    "    cdf = Cdf.from_dict(dict(weights))\n",
    "    indices = cdf.sample(len(weights))\n",
    "    return df.loc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to estimate the mean birthweight and compute SE and CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "estimates = np.array([\n",
    "    resample_rows_weighted(live).totalwgt_lb.mean() for _ in range(iters)\n",
    "])\n",
    "summarize(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the same calculation looks like if we ignore the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = np.array([\n",
    "    compstats.resample_rows(live).totalwgt_lb.mean() for _ in range(iters)\n",
    "])\n",
    "summarize(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the effect of weighting is small but non-negligible. The difference in estimated means, with and without weighting, is about 0.08 pounds, or 1.3 ounces. This difference is substantially larger than the standard error of the estimate, 0.014 pounds, which implies that the difference is not due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "**Exercise:** Using the data from the BRFSS, compute the linear least squares fit for log(weight) versus height. How would you best present the estimated parameters for a model like this where one of the variables is log-transformed? If you were trying to guess someone’s weight, how much would it help to know their height?\n",
    "\n",
    "Like the NSFG, the BRFSS oversamples some groups and provides a sampling weight for each respondent. In the BRFSS data, the variable name for these weights is totalwt. Use resampling, with and without weights, to estimate the mean height of respondents in the BRFSS, the standard error of the mean, and a 90% confidence interval. How much does correct weighting affect the estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the BRFSS data and extract heights and log weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = brfss.read_brfss().dropna(subset=['weight', 'height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda col: col.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_weight'] = np.log10(df.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate intercept and slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "inter, slope = least_squares(df.height, df.log_weight)\n",
    "np.round((inter, slope,), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatter plot of the data and show the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter + 75 * slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='height',\n",
    "    y='log_weight',\n",
    "    alpha=0.01,\n",
    "    s=5\n",
    ")\n",
    "plt.axline(\n",
    "    (75, inter + 75*slope),\n",
    "    slope=slope,\n",
    "    color='darkred', linestyle='--')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('log10 weight (kg)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the same plot but apply the inverse transform to show weights on a linear (not log) scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxs, fys = fit_line(df.height, inter, slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='height',\n",
    "    y='weight',\n",
    "    alpha=0.01,\n",
    "    s=5\n",
    ")\n",
    "plt.plot(fxs, 10**fys, color='darkred', linestyle='--')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('log10 weight (kg)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot percentiles of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['residual'] = residuals(df.height, df.log_weight, inter, slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(130, 210, 5)\n",
    "indices = np.digitize(df.height, bins)\n",
    "groups = df.groupby(indices)\n",
    "\n",
    "means = [group.height.mean() for i, group in groups][1:-1]\n",
    "cdfs = [Cdf.from_seq(group.residual) for i, group in groups][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percent in [75, 50, 25]:\n",
    "    ys = [cdf.percentile(percent) for cdf in cdfs]\n",
    "    plt.plot(means, ys, label=f'{percent:d}th')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('residual weight (kg)');\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "rho = compstats.corr(df.height, df.log_weight)\n",
    "np.round(rho, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "r2 = coef_determination(df.log_weight, df.residual)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that $R^2 = \\rho^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "np.round(rho**2 - r2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `std(ys)`, which is the RMSE of predictions that don't use height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "std_ys = compstats.std(df.log_weight)\n",
    "std_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute std(res), the RMSE of predictions that do use height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "std_res = compstats.std(df.residual)\n",
    "std_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does height information reduce RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "1 - std_res / std_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use resampling to compute sampling distributions for inter and slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "t = []\n",
    "for _ in range(100):\n",
    "    sample = compstats.resample_rows(df)\n",
    "    estimates = least_squares(sample.height, np.log10(sample.weight))\n",
    "    t.append(estimates)\n",
    "\n",
    "inters, slopes = zip(*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sampling distribution of slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(\n",
    "    x = slopes\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the p-value of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "cdf = Cdf.from_seq(slopes)\n",
    "pvalue = cdf.prob(0)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the 90% confidence interval of slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(cdf.values((0.05, 0.95)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean of the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "mean = np.mean(slopes)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the standard deviation of the sampling distribution, which is the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "stderr = compstats.std(np.array(slopes))\n",
    "stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample rows without weights, compute mean height, and summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_unweighted = np.array([\n",
    "    compstats.resample_rows(df).height.mean() for _ in range(100)\n",
    "])\n",
    "summarize(estimates_unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample rows with weights.  Note that the weight column in this dataset is called `finalwt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "estimates_weighted = np.array([\n",
    "    resample_rows_weighted(df, 'finalwt').height.mean() for _ in range(100)\n",
    "])\n",
    "summarize(estimates_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated mean height is almost 2 cm taller if we take into account the sampling weights, and this difference is much bigger than the sampling error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
